# 单文档关键词抽取算法介绍

## 1. TextRank算法

### 算法原理

TextRank算法是基于图的排序算法，原理是相邻节点之间相互打分，然后不断迭代打分过程，直到相邻两次迭代间各个节点之间的分值之差收敛到指定阈值或者迭代次数达到指定次数为止，最后按得分逆序排序节点所代表的词序列，位于序列前段的单词便是本文所需的关键词。

具体过程是：

	1. 首先对原文档做分句、去除特殊符号、分词和去除停用词处理；
	2. 然后将所有单词排成一个序列，在指定窗口（window）内的所有单词，都是图中的节点；
	3. 且位于窗口前端的单词都有一条出链指向窗口内其后边的单词，而该单词后面的单词都有一条入链指向它自己；
	4. 随着窗口在单词序列上的滑动，图中的节点在不断增加，节点与节点之间的边也在增加；
	5. 当窗口滑动到单词序列的末端时，这个图便构造完成。值得注意的是，由于有窗口的限制，因此构造的图是有向的；
	6. 接下来，只需要按照如下公式计算每个单词的得分，最终得到指定文档的关键词：


$$ WS(V_{i}) = (1-d) + d * \sum_{ V_{j} \in ln(V_{i})} \frac{\omega_{ji}}{\sum_{V_{k} \in Out(V_{j}) \omega_{jk}}}WS(V_{j}) $$


\\( WS(V_{i}) \\) 表示单词i的score；\\( d \\)是阻尼系数，一般取0.85，它在PageRank中表示用户将会以1-d的概率随机选择某个网页作为下一个网页，这并不考虑网页之间的连接关系；\\( Out(V_{j}) \\)表示单词j的所有出链节点构成的集合；\\( ln(V_{i}) \\)表示单词i的所有入链节点构成的集合；\\( \omega_{ji} \\)表示单词j指向单词i的连接权重。

### 相关论文

TextRank: Bringing Order into Texts

## 2. Rake算法

### 算法原理

Rake是Rapid Automatic keyword extraction的简称，它通过给每个候选的关键词打分，然后排序，得到最后的关键词。其原理是通过累加关键词中每个字的得分来求该关键词的得分，而每个字的得分由该字的度/该字的词频得到。每个字的度是指该字与文档中所有字在候选关键词中的共现次数，具体计算方式为，该字每与一个字共现在一个候选关键词中，度就加1，考虑该字本身。

### 2.1 Rake_raw.py流程

Rake_raw.py 遵循论文中的思想，以停用词来切割句子生成候选关键词

具体过程如下：

	1. 首先使用标点符号，如句号、问号、感叹号、逗号、分号，将一篇文档分成若干句
	2. 然后，对于每一个分句，使用停用词作为分隔符将句子分成若干个短语
	3. 这些短语便是待排序的候选词
	4. 每个短语都是由若干个字组成的，我们为每个字赋予一个得分，通过累加得到每个短语的得分
	5. 每个字的得分，由该字的度/该字的词频得到
	6. 每个字的度的计算方式如下，该字每与一个字共现在一个短语中，度就加1，考虑该字本身
	7. 对这些短语打分，然后排序，最后输出得分最高的几个短语作为关键词
	8. 每个字的度的计算方式如下:

$$wordScore_{w} = \frac{wordDegree_{w}}{wordFrequency_{w}}$$

### 2.2 Rake_split.py流程

Rake_split.py 通过对句子进行结巴分词，并去除停用词来生成候选关键词

具体过程如下：

	1. 首先使用标点符号，如句号、问号、感叹号、逗号、分号，将一篇文档分成若干句
	2. 然后，对于每一个分句，使用结巴分词，并去除停用词，来生成若干个候选短语
	3. 这些短语便是待排序的候选词
	4. 每个短语都是由若干个字组成的，我们为每个字赋予一个得分，通过累加得到每个短语的得分
	5. 每个字的得分，由该字的度/该字的词频得到
	6. 每个字的度的计算方式如下，该字每与一个字共现在一个短语中，度就加1，考虑该字本身
	7. 对这些短语打分，然后排序，最后输出得分最高的几个短语作为关键词
	8. 每个字的度的计算方式如下:

$$wordScore_{w} = \frac{wordDegree_{w}}{wordFrequency_{w}}$$

### 相关论文

Stuart Rose, Dave Engel, Nick Cramer,et al.Automatic keyword extraction from individual documents.Text Mining,2010:1-20

## 3. TF-IDF算法

### 算法原理

TF-IDF算法，通过计算文档中每个单词的TF-IDF值，然后输出TF-IDF值大的候选词作为关键词。TF-IDF是单词的 词频-逆文档频率 ，TF是词频，IDF是逆文档频率。这里，我们首先计算**LCSTS2.0**语料中每个单词的IDF值，然后再计算文档中每个单词的词频TF，最后计算文档中每个单词的TF-IDF值。

**LCSTS2.0** 是由HIT构建的短文本摘要的中文语料，其数据来源于sina微博，共有2412363篇文本。

### 单文档的TF-IDF算法

需要额外提供每个单词的IDF值字典

### 多文档的TF-IDF算法

不需要提供每个单词的IDF值字典，通过计算多文档语料库而得

## 4. 基于BERT的词聚类关键词抽取

### 算法原理

首先将文档进行分词，获取候选关键词，用于选取聚类中心词；然后将文档中的词进行聚类并获取聚类中心词；对文档进行词性标注，基于一定的模式(比如，选取名词短语)获取候选短语；最后，选择包含一个或多个聚类中心的短语作为最终的关键词。

### 算法流程

	1. 获取候选词：去除停用词，为关键词选取合适的候选聚类中心词
	2. 计算候选词之间的语义相似度: 欧式距离或余弦相似度
	3. 根据语义相似度对候选词进行聚类：利用BERT预训练的中文词向量作为度量词与词之间相似度的基础
	4. 选取每个聚类中心词作为种子词
	5. 根据Hulth的研究结论，大部分手工标注的关键词是名词短语。
	   因此，首先将文档进行词性标注，然后选取模式为 0个或多个形容词跟随1个或者多个名词的词串作为名词短语，将其作为候选关键词。
	6. 扩展关键词：从这些名词短语中选取那些包含一个或者多个聚类中心词的短语作为最终的关键词；

### 基于欧式距离的聚类算法: BERT_Clustering_distance.py

聚类算法的实现基于欧式距离

### 基于余弦相似度的聚类算法: BERT_Clustering_cosine.py

聚类算法的实现基于余弦相似度

## 5. 基于互信息的关键词抽取算法

### 相似度度量方法对关键词抽取的影响

### 聚类方法对关键词抽取的影响


### 相关论文

Hulth A. Improved automatic keyword extraction given more linguistic knowledge. Proceedings of EMNLP, 2003. 216–223

## 以上关键词抽取算法的比较

TextRank算法和Rake算法都无需训练，在单文本上就可以抽取关键词，但Rake算法倾向于抽取较长的关键词。

## 多文档关键词抽取算法介绍